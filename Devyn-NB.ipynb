{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a data directory and copy .env and functions.py\n",
    "\n",
    "datadir = 'data/'\n",
    "# !mkdir {datadir}\n",
    "# !cp ../repositories/da-external-data-sourcing/sql_functions.py . \n",
    "# !cp ../repositories/da-external-data-sourcing/.env .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_df = pd.read_csv('dataset/DataAnalyst.csv')\n",
    "scientist_df = pd.read_csv('dataset/DataScientist.csv')\n",
    "jobs_df = pd.read_csv('dataset/gsearch_jobs 3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(analyst_df.info())\n",
    "display(scientist_df.info())\n",
    "display(jobs_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(jobs_df.work_from_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(analyst_df[\"Job Description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Martin and Nuel's data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename Columns to lower and replace white spaces with Underscore\n",
    "analyst_df.rename(columns=lambda x : x.lower(), inplace=True)\n",
    "analyst_df.rename(columns=lambda x : x.replace(' ', '_'), inplace=True)\n",
    "scientist_df.rename(columns=lambda x : x.lower(), inplace=True)\n",
    "scientist_df.rename(columns=lambda x : x.replace(' ', '_'), inplace=True)\n",
    "jobs_df.rename(columns=lambda x : x.lower(), inplace=True)\n",
    "jobs_df.rename(columns=lambda x : x.replace(' ', '_'), inplace=True)\n",
    "\n",
    "jobs_df.rename(columns= {'title':'job_title', 'description':'job_description', 'description':'job_description'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientist_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns\n",
    "data_analyst_df2 = analyst_df.drop(columns= ['unnamed:_0', 'rating', 'company_name', 'location', 'headquarters',\n",
    "                                             'founded', 'competitors', 'easy_apply' ], axis=1)\n",
    "data_scientist_df2 = scientist_df.drop(columns= ['unnamed:_0', 'rating', 'company_name', 'location',\n",
    "                                                 'headquarters', 'founded', 'competitors', 'easy_apply'\n",
    "                                                ], axis=1)\n",
    "google_job_df2 = jobs_df.drop(columns= ['unnamed:_0', 'company_name', 'location', 'via', 'extensions',\n",
    "                                        'job_id', 'thumbnail', 'posted_at', 'search_term', 'date_time',\n",
    "                                        'search_location', 'commute_time', 'salary_pay', 'salary_rate',\n",
    "                                        'salary_avg', 'salary_min', 'salary_max', 'salary_hourly',\n",
    "                                        'salary_yearly', 'salary_standardized'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_df[\"job_description\"] = analyst_df[\"job_description\"].str.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('punkt')   # Required for tokenization\n",
    "#nltk.download('wordnet') # Required for lemmatization\n",
    "# nltk.download('popular')\n",
    "from nltk.corpus import stopwords\n",
    "# Instantiate\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Create our own stop words\n",
    "stop_words = (                                        \\\n",
    "    \"\\n ’ sleeves company and of for in to a or years l   \\\n",
    "    other were that data to drive policy we at with our   \\\n",
    "    firm follows under consolidated have these over   \\\n",
    "    include billion million december which an le from \\\n",
    "    certain le — million had show apple including     \\\n",
    "    as noncurrent current total three are on not is   \\\n",
    "    involving millions shows centers contains      \"  \\\n",
    "             ).split()\n",
    "# Exclusion list of punctuations and numbers\n",
    "exclist = string.punctuation + string.digits\n",
    "# Print the exclusion list\n",
    "print(exclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Function\n",
    "def clean_texts(text):\n",
    "    \"\"\" Function to perform preprocessing \"\"\"\n",
    "    \n",
    "    # Convert to lower cases\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuations and numbers\n",
    "    text = text.translate(str.maketrans(\"\", \"\", exclist))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "        \n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Join tokens\n",
    "    clean_text = \" \".join(tokens)\n",
    "    \n",
    "    # Return the output\n",
    "    return clean_text\n",
    "# Apply the function to all disclosures\n",
    "analyst_df['job_description'] = analyst_df['job_description'].apply(clean_texts)\n",
    "# View the first 5 rows\n",
    "analyst_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_text = analyst_df[\"job_description\"][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I needed to download the popular datasets/models in the command line with\n",
    "#  \"python -m nltk.downloader popular\"\n",
    "word_tokenize(zero_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(zero_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried to follow https://realpython.com/nltk-nlp-python/ for using NLTK\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# stop_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in zero_text:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_list = [\n",
    "#     word for word in zero_text if word.casefold() not in stop_words\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_df['description_words'] = analyst_df[\"job_description\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_df['description_words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_df['description_words'] = analyst_df['description_words'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_df['description_words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.FreqDist(analyst_df['description_words'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_keywords = [\"analyst\", \"python\"] # we want to keep # + C\n",
    "my_languages = [\"abap\", \"actionscript\", \"ada\", \"algol\", \"alice\", \"apl\", \"asp\", \"aspnet\", \"assembly\" \"language\",\n",
    "                \"assembly language\", \"awk\", \"bbc\", \"bbcbasic\", \"bbc basic\", \"c\", \"c++\", \"c#\", \"cobol\", \"d\", \"delphi\",\n",
    "                \"dreamweaver\", \"erlang\", \"elixir\", \"erlang and elixir\", \"f#\", \"forth\", \"fortran\", \"functional\", \"programming\",\n",
    "                \"functional programming\", \"go\", \"haskell\", \"html\", \"idl\", \"idl\", \"intercal\", \"java\", \"javascript\", \"node\", \"js\",\n",
    "                \"node.js\", \"jquery\", \"bootstrap\" \"labview\",\"lisp\",\"metaquotes language\",\"metaquoteslanguage\",\"ml\",\"modula-3\",\n",
    "                \"msaccess\",\"ms access\",\"mysql\",\"nxt-g\",\"object-oriented programming\",\"objective-c\",\"ocaml\",\"pascal\",\"perl\",\n",
    "                \"php\",\"pl/sql\",\"postgresql\",\"postscript\",\"prolog\",\"puredata\",\"python\",\"r\",\"rapidweaver\",\"ravendb\",\"rexx\",\n",
    "                \"ruby on rails\",\"Rubyonrails\",\"s-plus\",\"sas\",\"scala\",\"sed\",\"sgml\",\"simula\",\"smalltalk\",\"smil\",\"snobol\",\n",
    "                \"sql\",\"sqlite\",\"ssi\",\"stata\",\"swift\",\"tcl/tk\",\"tex and latex\",\"texandlatex\",\"unified modeling language\",\n",
    "                \"unix shells\",\"unixshells\",\"verilog\",\"vhdl\",\"visualbasi\",\"visual basis\",\"vrml\",\"wap/wml\",\"xml\",\"xsl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_df['description_words'] = analyst_df['description_words'].apply(lambda x: [item for item in x if item in my_languages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_df['description_words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.FreqDist(analyst_df['description_words'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_list\n",
    "# analyst_df.dtypes\n",
    "# analyst_df = analyst_df[\"job_description\"].astype(str)\n",
    "# analyst_df.dtypes\n",
    "# analyst_df.str.split(pat=None, n=-1, expand=False)\n",
    "# analyst_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nf_sql_api')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f113de48747f474753e1019d5dacf17fb0593e28dec325b9669591d90321aa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
